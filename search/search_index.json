{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Avengercon VIII Workshop  Horizontally Scaling Python for Production","text":"<p>Repo Status  </p> <p>Tooling  </p> <p> </p> <p>Workshop Date: 28 February 2024</p> <p>The Avenercon VIII conference homepage has details about the event where this workshop was held.</p> <p>This is a hands-on Python programming workshop. At least one year of recent Python experience, some experience with Docker, and a computer you administer is strongly recommended.</p> <p>Python can be challenging to use in production when \"real-world\" workloads involving megabits per second (Mbps) of streaming data or terabytes of stored data are involved. The Global Interpreter Lock (GIL) means that a Python interpreter is effectively single-threaded and can't take advantage of modern processors' capacity for parallel computation. Laterally scaling workloads across many Python interpreters is one of the most viable workarounds to the shortcomings of the GIL. This workshop will introduce you to two leading frameworks for doing this: Celery, Dask, and Apache Beam. This workshop will walk through establishing an Extract, Transform, Load (ETL) pipeline in each framework which reads and writes from Redis and a MinIO locally hosted S3 bucket.</p>"},{"location":"developers/cloudflare/","title":"Cloudflare","text":""},{"location":"developers/cloudflare/#cors","title":"CORS","text":"<p>Configuring CORS in cloudflare documentation covers several methods, particularly with CORS pre-flight requests, for effectively controlling CORS and preventing issues.</p>"},{"location":"developers/cloudflare/#application-audience-aud-tag","title":"Application Audience (AUD) Tag","text":"<p>Cloudflare Access applications have a dynamically generated AUD tag which will need to be used by any services (e.g. API) to authenticate a Cloudflare Access generated JWT.</p> <p>The online documentation describes where to find the AUD with examples of decoding Cloudflare's JWTs in Go, Python, and Javascript.</p>"},{"location":"developers/cloudflare/#team-domain","title":"Team Domain","text":"<p>The 'team domain' is another unique piece of information used by cloudflare's SDKs to do things like dynamically retrieve the appropriate client PKI keys for a cloudflare zero-trust application. This information can be retrieved from the zero-trust dashboard under <code>Settings &gt; Custom Pages</code>.</p>"},{"location":"developers/docker/","title":"Docker","text":""},{"location":"developers/docker/#testing-dockerfile-setups","title":"Testing <code>Dockerfile</code> setups","text":"<p>To quickly verify whether a Dockerfile is properly configured, try independently running <code>docker build</code> outside of docker compose.</p> <p>For example, with the Api.Dockerfile: <pre><code>docker build -t &lt;image_name&gt; -f &lt;dockerfile_path&gt; .\n</code></pre></p>"},{"location":"developers/docker/#docker-commands","title":"Docker Commands","text":"<ul> <li><code>docker exec -it [CONTAINER_ID] /bin/bash</code> or <code>/bin/sh</code> to get terminal a container</li> <li><code>docker ps</code> lists running containers, their status, and ports</li> <li>Remove 'everything' in Docker (images, etc.): <code>docker system prune -a</code></li> <li>Remove all containers: <code>docker container prune</code></li> <li>Remove all volumes: <code>docker volume prune</code></li> <li>Remove all 'dangling' images: <code>docker rmi $(docker images -f \"dangling=true\" -q)</code></li> <li>List containers: <code>docker container ps --all</code></li> <li>Stop container: <code>docker stop &lt;container-id&gt;</code></li> <li>Delete stopped container: <code>docker rm &lt;container-id&gt;</code></li> <li>List images: <code>docker images</code></li> <li>Delete image: <code>docker image rm &lt;image-id&gt;</code></li> <li>Docker Compose Anchors and Extensions</li> </ul>"},{"location":"developers/docker/#docker-compose","title":"Docker Compose","text":"<p>Docker compose 2 looks for <code>docker-compose.yml</code> to begin orchestration. However, it then also looks at <code>docker-compose.override.yml</code> to override/merge the settings from the base configuration. See the merge compose files documentation for the detailed rules on how this happens.</p>"},{"location":"developers/docs/","title":"Documentation","text":""},{"location":"developers/docs/#mkdocs-material","title":"mkdocs-material","text":"<p>This project uses mkdocs-material to manage and dynamically generate documentation as a set of static websites.</p> <p>To generate and view the compiled documentation locally, run the following within the activated poetry virtual environment:</p> <p><pre><code>mkdocs build\n</code></pre> <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"developers/docs/#admonitions","title":"Admonitions","text":"<p>The admonitions extension is used to provide callouts. The list of available admonitions is available to customize callouts.</p> <p>Examples:</p> <pre><code>!!! note\n    This is a note.\n</code></pre> <p>Note</p> <p>This is a note.</p> <pre><code>!!! warning\n    This is a warning.\n</code></pre> <p>Warning</p> <p>This is a warning.</p>"},{"location":"developers/docs/#buttons-and-icons","title":"Buttons and Icons","text":"<p>The icon search page has a quick lookup of available icons that mkdocs-material supports.</p> <pre><code>[Send :fontawesome-solid-paper-plane:](#){ .md-button }\n</code></pre> <p>Send </p>"},{"location":"developers/docs/#example-sites-using-mkdocs-material","title":"Example Sites Using Mkdocs-Material","text":"<ul> <li>up42 is a nice example of customizing mkdocs-material defaults.</li> </ul>"},{"location":"developers/gitlab_ci/","title":"GitLab CI","text":"<p>Gitlab Continuous Integration (CI) is a way for automatically testing and building artifacts when commits are made to certain branches. For example, base container images, the mkdocs-material documentation hosted on GitLab Pages, etc.</p>"},{"location":"developers/gitlab_ci/#testing-ci-locally","title":"Testing CI locally","text":"<p>The gitlab-runner is available as both an installable binary or docker image.</p> <p>To test a CI pipeline before committing (and thus triggering a 'real' CI job), the gitlab-runner can be used locally.</p> <p>Locally installed binary example run to generate mkdocs documentation site: <pre><code>gitlab-runner exec docker .pages-localhost\n</code></pre></p> <p>Docker-in-docker deployment: <pre><code>docker run gitlab/gitlab-runner exec docker .pages-localhost\n</code></pre></p> <p>Note</p> <p>Additional details of publishing mkdocs-material generated sites to GitLab is available here.</p>"},{"location":"developers/gitlab_ci/#using-ci-to-build-and-pushing-images-to-the-container-registry","title":"Using CI to build and pushing images to the container registry","text":"<p>To use GitLab CI/CD to build and push images, this documentation walks through each step of the process.</p>"},{"location":"developers/gitlab_ci/#manually-publishing-images-to-the-container-registry","title":"Manually publishing images to the container registry","text":"<ul> <li>See this page for documentation.</li> <li>Look for the \"CLI Commands\" button in top right corner of project's Packages &amp; registries menu.</li> <li><code>docker login registry.gitlab.com</code></li> <li>Use a personal access token for your password</li> <li>Use a URL style tag for the image then build it.</li> <li><code>docker build -t registry.gitlab.com/my-group/my-project/&lt;image_name&gt; .</code></li> <li>Use the URL style tag to then push to the container registry.</li> <li><code>docker push registry.gitlab.com/my-group/my-project/&lt;container_name&gt;</code></li> </ul>"},{"location":"developers/os/","title":"Operating Systems","text":""},{"location":"developers/os/#general-os-configurations","title":"General OS Configurations","text":""},{"location":"developers/os/#windows","title":"Windows","text":"<p>Warning</p> <p>Update your git settings so all files are not auto-converted to Windows style line endings: <code>git config --global core.autocrlf false</code></p>"},{"location":"developers/os/#docker-buildkit","title":"Docker Buildkit","text":"<p>Enabling Docker BuildKit will help ensure pip and npm aren't constantly downloading the same packages over and over again.    - On your host machine, enable BuildKit using an environment variable: <code>export DOCKER_BUILDKIT=1</code>    - To permanently set the DOCKER_BUILDKIT flag on Ubuntu:      - <code>gedit ~/.profile</code>      - Add the following to the .profile: <code>export DOCKER_BUILDKIT=1</code></p>"},{"location":"developers/os/#available-scripts","title":"Available Scripts","text":"<p>Be sure to mark scripts as executable with <code>chmod +x &lt;script_path&gt;</code></p> <ul> <li><code>run_dev.sh</code> runs a Docker Compose deployment in a Development configuration.</li> <li><code>soft_reset.sh</code> removes containers and resets <code>.env</code> configs. Volumes and stateful authentication credentials such as passwords are retained.</li> <li><code>full_reset.sh</code> removes containers and resets <code>.env</code> configs. Volumes are deleted.</li> <li><code>initialize_env.sh</code> attempts to create a new <code>.env</code> deployment config.</li> <li><code>export_poetry_to_req_txt.[sh|bat]</code> use poetry to export the various Python environment dependencies to <code>requirements.txt</code> format compatible with <code>pip</code>.</li> <li><code>update_tooling.[sh|bat]</code> attempts to run self-update features of the recommended dev tooling.</li> <li><code>setup_linux_kvm_amd.sh</code> helps set up a Docker user and virtualization on Linux</li> <li><code>force_poetry_shell.sh</code> helps activate a poetry virtual environment in the current terminal when <code>poetry shell</code> isn't working.</li> </ul>"},{"location":"developers/postgres/","title":"PostgreSQL","text":""},{"location":"developers/postgres/#postgresql-container-management","title":"PostgreSQL Container Management","text":""},{"location":"developers/postgres/#troubleshooting-postgres-container-authentication","title":"Troubleshooting Postgres container authentication","text":"<ul> <li>It's necessary to prune both containers and volumes related to postgres to easily reset the initial settings.</li> <li>You'll know you adequately reset the postgres container build to grab new settings if you see <code>CREATE DATABASE</code> and extraordinarily long output generated when starting the container.</li> <li>To see the values of a custom enumerated type in Adminer or postgres command line, run the following sql query: <code>SELECT enum_range(NULL::&lt;custom_type_name&gt;)</code></li> </ul>"},{"location":"developers/sqlalchemy_alembic/","title":"SQLAlchemy & Alembic","text":""},{"location":"developers/sqlalchemy_alembic/#alembic-database-migration-tips","title":"Alembic Database Migration tips","text":"<ul> <li>Auto-generate a database revision</li> <li><code>alembic revision --autogenerate -m \"Added table XYZ\"</code></li> <li>Look in alembic &gt; versions for the revision and validate/update as needed.</li> <li>Implement the revision with <code>alembic upgrade head</code></li> </ul>"},{"location":"developers/sqlalchemy_alembic/#sqlalchemy-mapped-class","title":"SQLAlchemy Mapped Class","text":"<p>The walkthrough of SQLAlchemy 2.0 Declarative Mapping helps quickstart correct design pattern understanding.</p> <ol> <li>Declaring a One-to-Many Relationship with cascading deletion.</li> <li>Declaring a Many-to-Many relationship with unique constraints.</li> <li>Use mixins for recurring column name/types like <code>updated_on</code> datetimes.</li> </ol> <p>Note</p> <p>The mixin example references <code>func.now()</code> which isn't pseudocode. Use <code>from sqlalchemy import func</code>.</p> <ol> <li>Combining dataclass &amp; mixins features</li> </ol>"},{"location":"developers/sqlalchemy_alembic/#sqlalchemy-types","title":"SQLAlchemy Types","text":"<p>The SQLAlchemy Type system supports both generic and backend specific types.</p> <p><code>mapped_column()</code> has implied associations between SQLAlchemy types and basic Python types which can be customized.</p>"},{"location":"developers/testing/","title":"Testing","text":""},{"location":"developers/testing/#pytest-tips","title":"Pytest tips","text":"<p>See the <code>pyproject.toml</code> and <code>tox.ini</code> in the source code for a pytest configuration reference.</p> <ul> <li> <p>Run a specific test:</p> <ul> <li><code>pytest tests/basic_test.py -k \"test_import\" -vv</code></li> <li><code>pytest tests/basic_test.py::test_import</code></li> </ul> </li> <li> <p>Run a test with benchmarking (requires pytest-benchmark)</p> <ul> <li><code>pytest tests/vertical_scale_test.py --benchmark-histogram</code></li> </ul> </li> </ul>"},{"location":"developers/traefik/","title":"Traefik","text":""},{"location":"developers/traefik/#configuring-traefik-securely","title":"Configuring Traefik securely","text":"<p>Mozilla SSL Configuration Generator is a nice tool for seeing good default configuration options.</p>"},{"location":"workshop/1_hello_workshop/","title":"Hello, Workshop!","text":"<p>This is the beginning of our journey. Our goal is simple but not always easy: can we get our developer environment setup?</p> <p></p>"},{"location":"workshop/1_hello_workshop/#get-ready-system-level-installation","title":"Get Ready: System-level installation","text":"<p>You'll need the following software installed to begin:</p> <ol> <li>Docker</li> <li>Python 3.11</li> <li>Git</li> </ol> <p>Optional software</p> Python interpreter managerPoetryGit GUI Client <p>To keep Python from messing up your OS configs, It is recommended to install &amp; manage Python interpreters using a manager like pyenv or hatch. This will allow you to add/remove specific and multiple Python environments on your system. Testing your code against multiple versions of Python is a necessity for professional projects!</p> <p>Poetry is an alternative to <code>pip</code>. Those who prefer <code>pip</code> or other Python environment tools are welcome to use the <code>requirements-dev.txt</code> file in the requirements directory of this repo. Why Poetry? While hatch and other tools are promising, as of this workshop Poetry remains one of the best developer experiences (DX) for setup of python virtual environments and modern pyproject.toml based Python packages.</p> <p>Git GUI Clients simply save time. While it may feel nice to flex that you have every possible permutation of Git commands memorized, 2-second point-and-click to stage dozens of files and side-by-side highlighted diffs are the way to go.</p>"},{"location":"workshop/1_hello_workshop/#get-set-prepare-your-virtual-environment","title":"Get Set: Prepare your virtual environment","text":""},{"location":"workshop/1_hello_workshop/#install-the-avengercon-package-and-dependencies","title":"Install the <code>avengercon</code> package and dependencies","text":"<ol> <li>Establish a virtual environment with <code>poetry shell</code> or use pip</li> <li>Install <code>avengercon</code> &amp; dependencies with <code>poetry install</code> or use pip</li> <li>Confirm that the local <code>avengercon</code> package is available in your virtual environment</li> </ol> <pre><code>(avengercon-py3.11) $ python -m avengercon\nHello, Workshop!\n</code></pre>"},{"location":"workshop/1_hello_workshop/#configure-your-ide-to-use-the-virtual-environment","title":"Configure your IDE to use the virtual environment","text":"<p>VSCode, PyCharm, and most other popular Integrated Development Environment (IDEs) support intellisense and other productivity boosters when properly configured to use the project's virtual environment.</p>"},{"location":"workshop/1_hello_workshop/#generate-your-env-file","title":"Generate your <code>.env</code> file","text":"<p>Using a terminal from the top level <code>avengercon_2024</code> directory, run the <code>initialize_env.sh</code> script to dynamically generate configuration metadata in a <code>.env</code> file.</p> WindowsUnix (Mac/Linux) <pre><code>bash scripts\\initialize_env.sh\n</code></pre> <pre><code>chmox +x scripts/initialize_env.sh\n./scripts/initialize_env.sh\n</code></pre> <p>You should now see a <code>.env</code> file in your <code>avengercon_2024</code> directory that looks something like this: .env<pre><code># Used by run_*.sh scripts &amp; avengercon module to dynamically configure localhost\n# development and testing environment variable coordination.\n# Valid log level values: 'critical', 'error', 'warning', 'info', 'debug'\nLOG_LEVEL=info\n# Traefik settings and labels\nHTTP_PORT=57073\nDOMAIN=localhost\n...\n</code></pre></p>"},{"location":"workshop/1_hello_workshop/#launch","title":"\ud83d\ude80 Launch!","text":"<p>Using a terminal from the top level <code>avengercon_2024</code> directory, launch the <code>docker compose</code> deployment using the <code>run_dev.sh</code> script.</p> WindowsUnix (Mac/Linux) <pre><code>bash run_dev.sh\n</code></pre> <pre><code>chmox +x run_dev.sh\nrun_dev.sh\n</code></pre> <p>Keep your containers deployed</p> <p>Don't close the terminal running your deployment! We'll be using the logs that appear to help monitor the state of our workshop's services and code.</p> <p>Port conflicts</p> <p>If you're already running a Traefik reverse proxy or services that use the ports listed in the <code>.env</code>, you will need to adjust the ports being used for this workshop.</p> <p>If everything is going well, you should not see any \"ERR\" or \"ERROR\" in the logs that appear in your terminal. Happy Logs<pre><code>...\navengercon-minio   | Documentation: https://min.io/docs/minio/linux/index.html\navengercon-minio   | Warning: The standard parity is set to 0. This can lead to data loss.\navengercon-proxy   | 2024-02-15T16:35:39Z INF Traefik version 3.0.0-rc1 built on 2024-02-13T13:41:20Z version=3.0.0-rc1\navengercon-proxy   | 2024-02-15T16:35:39Z INF\navengercon-proxy   | Stats collection is disabled.\navengercon-proxy   | Help us improve Traefik by turning this feature on :)\navengercon-proxy   | More details on: https://doc.traefik.io/traefik/contributing/data-collection/\navengercon-proxy   |\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider aggregator aggregator.ProviderAggregator\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider *docker.Provider\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider *traefik.Provider\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider *acme.ChallengeTLSALPN\navengercon-minio   |\navengercon-minio   |  You are running an older version of MinIO released 1 day before the latest release\navengercon-minio   |  Update: Run `mc admin update ALIAS`\navengercon-minio   |\navengercon-minio   |\navengercon-whoami  | 2024/02/15 16:38:02 172.27.0.3:45642 - - [15/Feb/2024:16:38:02 +0000] \"GET / HTTP/1.1\" - -\n...\n</code></pre></p>"},{"location":"workshop/1_hello_workshop/#verify-you-can-access-your-deployed-services","title":"Verify you can access your deployed services","text":"<p>Ensure you can open a web browser to the following local services. For each button, you probably want to right-click and open in new tab.</p> <p>If anything fails to open, double-check the port in the opened link matches the port specified by <code>HTTP_PORT</code> in your <code>.env</code> configuration. (default is <code>57073</code>; l33t speak approximation for \"Stone\")</p> WhoamiRedisMinIODaskDask NotebookPrefectCelery (Flower)Swagger (FastAPI)Traefik <p>Whoami </p> whoami.localhost<pre><code>Name: avengercon_whoami\nHostname: b967780eb9c6\nIP: 127.0.0.1\nIP: ###.###.###.###\nRemoteAddr: ###.###.###.###:42012\nGET / HTTP/1.1\nHost: whoami.localhost:57073\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\nAccept-Encoding: gzip, deflate, br\n...\n</code></pre> <p>Redis </p> <p>The login is <code>default</code> and the password is in your <code>.env</code> as the <code>REDIS_PASSWORD</code> value</p> <p></p> <p>MinIO </p> <p>The login and password is in your <code>.env</code> as the <code>MINIO_ROOT_USER</code> and <code>MINIO_ROOT_PASSWORD</code> values</p> <p></p> <p>Dask </p> <p></p> <p>Jupyter Notebook </p> <p>Your login token will be listed in the terminal next to an <code>avengercon-dask-notebook</code> log entry. You'll need to copy-paste just the token portion of the url</p> <pre><code>avengercon-dask-notebook       | [... ServerApp] Jupyter Server 2.12.5 is running at:\navengercon-dask-notebook       | [... ServerApp] http://fd52fdf68911:8888/lab?token=b952e22de792f69923d281c04f66393518cd74a0c7fd1acf\n                                            EVERYTHING AFTER THE = IS YOUR TOKEN    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\navengercon-dask-notebook       | [I 2024-02-17 19:42:45.576 ServerApp]     http://127.0.0.1:8888/lab?token=b952e22de792f69923d281c04f66393518cd74a0c7fd1acf\navengercon-dask-notebook       | [I 2024-02-17 19:42:45.576 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre> <p></p> <p>Prefect </p> <p></p> <p>Celery </p> <p></p> <p>FastAPI  + </p> <p></p> <p>Traefik </p> <p></p>"},{"location":"workshop/2_pros_cons/","title":"Parallel Pros and Cons","text":""},{"location":"workshop/2_pros_cons/#python-parallelization-frameworks","title":"Python Parallelization Frameworks","text":"<p>We'll be exploring four different methods for horizontally scaling python in this workshop:</p> <ul> <li> <p> Python multiprocessing</p> <p>For when you just need one or two functions to scale. Chances are good that if you try to make anything non-trivial, you're going to invest weeks/months of effort to discover you've made a junk version of Dask or Celery released 10 years ago.</p> <p>Self-authored multiprocessing</p> <p>Process #1: Knock, Knock</p> <p>Process #2: Whose thRACE CONDITION FROM PROCESS #1 BECAUSE YOU DIDN'T USE MULTIPROCESSING CORRECTLY</p> <p>Pros:</p> <p> Works \"out of the box\"</p> <p> Potential for batch &amp; streaming (good luck...)</p> <p>Cons:</p> <p> Race conditions are on you</p> <p> Memory management is on you</p> <p> Synchronization is on you</p> <p> Inter-process dataflow is on you</p> </li> <li> <p> Dask</p> <p>Use this if you're already planning on using Pandas. If you aren't using Pandas and all the benefits of optimized C outside of the GIL it brings, it's worth taking a pause to double-check you CAN'T use it before using something else.</p> <p>Pros:</p> <p> Nearly 1-for-1 API parity with standard Pandas</p> <p> Effortless scaling for Dataframe-based workflows</p> <p> Support for non-Dataframe tasks</p> <p> Painless infrastructure integration</p> <p> 1st class support for GPUs (via RAPIDS)</p> <p>Cons:</p> <p> Centered around Pandas (columnar data, sorry JSON)</p> <p> Complex workflows aren't a strong suit</p> <p> Getting custom code &amp; dependencies onto workers is a learning curve</p> <p> While streaming may be theoretically possible, it's built for batch workflows</p> </li> <li> <p> Celery</p> <p>The Python parallelization swiss-army knife. This can do whatever you're trying to do.</p> <p>Pros:</p> <p> Complex workflows are a specialty</p> <p> Integrating your project code &amp; dependencies is the default</p> <p> Quirky but relatively painless infrastructure</p> <p> Probably supports where you store your data</p> <p>Cons:</p> <p> Canvas (workflow) API has a learning curve</p> <p> Poor support for arbitrarily long tasks</p> <p> Inter-process JSON messages can be difficult to predict</p> <p>  Flower doesn't have a dark mode</p> </li> <li> <p> Apache Beam</p> <p>This is the endgame  If you're truly starting to scale but don't want to ditch Python, then your journey will probably lead here.</p> <p>Pros:</p> <p> Forces effective map-shuffle-reduce patterns</p> <p> Potentially fastest (with  Apache Flink) and scales harder than Chuck Norris can kick</p> <p> 1st class support for streaming dataflows and all the complexity that goes along with that (windowing, late arrivals, only once, at least once, etc.)</p> <p> Leverage existing infrax ( GCP,  Spark, etc.)</p> <p> Create effective Spark/Flink jobs with Python</p> <p>Cons:</p> <p> Just an abstraction layer (less the dev-only Direct Runner)</p> <p> Complex infrax setup for self-hosted prod deployment</p> <p> Semi-linked to GCP's DataFlow implementation</p> <p> Chained dependencies cause projects to be stuck with months old libraries</p> <p> Semi-locked in options for sources and sinks</p> </li> </ul>"},{"location":"workshop/2_pros_cons/#when-are-all-of-these-options-a-bad-idea","title":"When are all of these options a bad idea?","text":"<p>Spending weeks learning a new language is likely going to be slower than writing something in a language you already know today (CPython) and running it. That said, CPython is very upfront about its inability to use threading. Python 3.12+ is beginning the slow process of overcoming the Global Interpreter Lock (GIL) inability to support multiple threads. Details are in PEP 703.</p> <p>Until the GIL supports threads and the Python ecosystem (SciPy, Dask, FastAPI, etc.) adapts to the change, the best case scenario with Python is multiprocessing using orders of magnitude more memory, layers of complexity, and slightly more time to accomplish a task compared to what compiled languages with threading can accomplish with basic functions.</p>"},{"location":"workshop/2_pros_cons/#vertically-scaling-python","title":"Vertically scaling \"Python\"","text":"<p>While this workshop is focused on horizontally scaling Python, it's worth making some honorable mentions for vertically scaling individual Python interpreters to be more performant. The theme here is: speed up Python by minimizing the use of Python.</p> <ul> <li> <p> Wrap C &amp; C++ with Python</p> <p>This likely isn't new information, but directly extending Python with C or C++ is how Numpy, Pandas, and much of the CPython standard library is made.</p> <p>Pros:</p> <p> You're that dev who can optimize Python with C </p> <p>Cons:</p> <p> You're the dev trying to optimize Python with C </p> </li> <li> <p> Compile and Cache Python</p> <p>py_compile and functools.lru_cache are \"out of the box\" and relatively painless ways to speed up your critical path.</p> <p>Chances are pretty good that the Python interpreter is already compiling your code to <code>.pyc</code> files.</p> <p>Pros:</p> <p> 1,000x performance increase with 1-line of code and no added dependencies</p> <p>Cons:</p> <p> If it works and memory holds out </p> </li> <li> <p> Numba</p> <p>Step 1. Put <code>@jit</code> above <code>def my_function()</code></p> <p>Step 2. Magic</p> <p>Pros:</p> <p> Possibility of quick-win 1,000x or more performance increases for your project</p> <p> Junior devs will think you're an all knowing Python god for greatly speeding up the Python codebase</p> <p>Cons:</p> <p> If it works...</p> <p> About as likely as a used mattress to introduce </p> <p> Your time is probably better spent learning a threaded language rather than a bolt on solution for Python</p> <p> Senior devs will probably be annoyed you've increased the complexity/fragility of the codebase and bloated prod images</p> </li> <li> <p> Taichi</p> <p>You can install it with pip, write it in your <code>.py</code> files, and it looks like Python. BUT... you're not really using Python anymore. Similar situation as Numba:</p> <p>Step 1: Hop on a magic carpet with <code>ti.init(arch=ti.cpu)</code></p> <p>Step 2: Put <code>@ti.kernel</code> above your function.</p> <p>Step 3: Magic</p> <p>Pros/Cons:</p> <p>Similar tradeoffs as Numba</p> </li> </ul>"},{"location":"workshop/2_pros_cons/#taichi-advertised-benchmark","title":"Taichi advertised benchmark","text":""},{"location":"workshop/2_pros_cons/#plain-python-vs-lru_cache-vs-taichi-vs-numba","title":"Plain Python vs lru_cache vs Taichi vs Numba","text":"<p>Fibonacci Number Benchmarks</p> <p>The benchmark below is reproducible by running the standalone tests included in the workshop's <code>tests</code> directory:</p> <p><code>pytest tests/vertical_scale_test.py --benchmark-histogram</code></p> <p>Key observations from setting up a benchmark for a somewhat \"normal\" Python function:</p> <ol> <li> <p>For idempotent functions called \"a lot\" with similar input, adding <code>@lru_cache</code> above the function definition is almost certainly the best option.</p> </li> <li> <p>Just-in-time (JIT) compiled solutions (e.g. Taichi &amp; Numba) implicitly fail when reaching C max/min scalar sizes. Both silently failed when trying to compute numbers larger than the underlying C can support.</p> </li> <li> <p>Taichi is more \"honest\" about its limitations. Numba will implicitly fall back to Python without warning (e.g. the <code>fib_numba()</code> test function) when its assumptions (which in general are the same as Taichi's) aren't met.</p> </li> <li> <p>Added complexity to debug. For example Taichi requires explicitly turning on debugging in its setup: <code>taichi.init(debug=True)</code></p> </li> <li> <p>If you're writing custom mathematical computation functions AND those functions are a clear bottleneck for the project goals AND function input isn't expected to be repetitive (so cache hits won't help) AND the math can't be done using native numpy/pandas or machine learning library functions THEN it MAY make sense to look at optimization solutions like Numba or Taichi.</p> </li> </ol> <p></p>"},{"location":"workshop/3_dask/","title":"Dask","text":""},{"location":"workshop/3_dask/#what-is-dask","title":"What is Dask?","text":"<p>The Dask Tutorial and this article by NVIDIA has decent infographics and explanations on what Dask is. The VERY summarized explanation is it's a library that combines Tornado and Pandas so that an arbitrary number of Python interpreters and Pandas DataFrames can be used as if they were a single interpreter and DataFrame.</p> <p>The Journey of a Task explanation by the Dask authors provides a nice end-to-end primer on how the framework operates.</p>"},{"location":"workshop/3_dask/#what-is-coiled-and-prefect","title":"What is Coiled and Prefect?","text":"<p>Dask fits into a growing segment of the data/tech industry where Free and Open Source Software (FOSS) is provided with fully-managed and extended offerings made available by the primary contributors to make an income.</p> <p>Two of the more prominent companies aligned with Dask are Coiled.io and Prefect. Coiled is basically a fully-managed Dask cluster while Prefect is an expanded offering more geared towards ETL pipelines.</p>"},{"location":"workshop/3_dask/#dask-created-hands-on-crash-course","title":"Dask created hands-on crash course","text":"<p>Jupyter Notebook </p> <p>Transition to the official crash-course running on your computer to get comfortable with the framework.</p>"},{"location":"workshop/3_dask/#preparing-for-using-dask-in-your-own-projects","title":"Preparing for using Dask in your own projects","text":"<p>Since we've already seen some basics of using Dask in the Jupyter notebooks, let's transition to a couple of tasks using Prefect.</p>"},{"location":"workshop/3_dask/#open-the-prefect-user-interface","title":"Open the Prefect User Interface","text":"<p>Right-click and \"open in new tab\"</p> <p>Prefect </p>"},{"location":"workshop/3_dask/#integrating-prefect-with-aws-s3minio","title":"Integrating Prefect with AWS S3/MinIO","text":"<p>In your IDE with your virtual environment activated (as described earlier), try making and running a new python script in the <code>avengercon_2024</code> directory.</p> testing.py<pre><code>from avengercon.prefect.flows import hello_prefect_flow\nfrom avengercon.prefect.storage import create_default_prefect_blocks, create_default_prefect_buckets\n\nprint(hello_prefect_flow())\ncreate_default_prefect_buckets()\ncreate_default_prefect_blocks()\n</code></pre> <p>Take a look at the \"Blocks\" portion of the Prefect UI. You should see <code>prefect-artifacts</code> and <code>prefect-flows</code> as registered S3-like buckets. Clicking the link on either will show instructions on how to use these buckets in the future to cache both the files your team is working on and the code you're using to do so. This may be particularly helpful when your operators may want to trigger a pre-defined series of steps for new data by triggering a deployment that uses a flow the dev team stored in a block.</p>"},{"location":"workshop/4_celery/","title":"Celery & Flower","text":""},{"location":"workshop/4_celery/#what-is-celery","title":"What is Celery","text":"<p>Celery is a pure-Python implementation of what Dask calls bags or Prefect calls tasks and flows. It is designed to coordinate data among machines and processes using brokers like Redis, RabbitMQ, and Amazon SQS. It can also store the results of code directly to a backend you're probably already using such as AWS S3, Azure Blob, Elasticsearch, various databases, and plain files.</p> <p>It's a relatively simple framework compared to Dask or Apache Beam since it only requires Python to work. Like Dask, it has a scheduler helping work get passed to workers that actually run the code. Like Prefect, it's generally embedded into your own codebase using the <code>@task</code> decorator above your functions.</p>"},{"location":"workshop/4_celery/#what-is-flower","title":"What is Flower","text":"<p>right click and \"open in new tab\"</p> <p>Celery </p> <p>Flower is a companion project for Celery to provide a GUI for the scheduler similar to what we saw with Dask and Prefect (albeit not as pretty).</p>"},{"location":"workshop/4_celery/#getting-started-with-celery-the-canvas-api","title":"Getting started with Celery: the Canvas API","text":"<p>Celery calls its main workflow API canvas.</p> <p>The use guide does a good job kick-starting understanding but here is a quick summary of the ideas:</p> <ol> <li> <p>The Celery App is a long-running process that essentially starts the scheduler server. Here we'll call this run time Python object <code>app</code>.</p> </li> <li> <p>Individual Python functions are made into Celery Tasks by registering them with a decorator above the function declaration like <code>@app.task</code></p> </li> <li> <p>Chains are when there's a sequential series of steps you'd like placed together. These steps can fork &amp; recombine, represent a parallel computation (via Groups), an individual Task, etc. Each part of the chain can get all, some, or none of the output created by previous steps.</p> </li> <li> <p>Groups are for when there's multiple Tasks, Chains, other groups, etc. that aren't dependent on each other that you'd like to execute in parallel.</p> </li> </ol> <p>That's it!. There's more to the API but the vast majority of working with Celery is arranging Tasks into a Directed Acyclic Graph (DAG) via Chains and Groups.</p>"},{"location":"workshop/4_celery/#tips-for-working-with-the-celery","title":"Tips for working with the Celery","text":"<ol> <li> <p>Check out the Tips and Best Practices section of the documentation.</p> </li> <li> <p>Celery Tasks can't call Celery Tasks. They can't be \"nested\"</p> </li> <li> <p>Default behavior is for Celery tasks to have preset timeouts It's worth taking time to understand how to adjust timeouts for tasks.</p> </li> <li> <p>When using Chains, if a previous Task produces an output then the following Task will be passed that as in input. This isn't a bad thing but makes it tricky to craft your function declarations without doing some guess-and-check to determine exactly what gets passed between functions.</p> </li> <li> <p>Think of inter-task communication like a webpage calling back to a server. When using the default JSON serializer, it's important to realize that integers like 1 will arrive as the string \"1\". Also, Python objects like a Pandas Dataframe need to be converted to something that can be serialized into a string value in JSON.</p> </li> <li> <p>Unlike Dask or Apache Beam that help minimize the possibility of race conditions, be very careful when using Celery to work with a database or other \"source of truth\" shared by Tasks executing in parallel via a Group.</p> </li> <li> <p>Make sure the Celery Application \"sees\" your tasks. The first step is to add anywhere you've declared tasks with the <code>@app.task</code> decorated to the App's <code>include</code></p> </li> </ol> <p>Including tasks with the Celery App</p> avengercon.celery.tasks.py<pre><code>from avengercon.celery import celery_server\n\n@celery_server.task(name=\"tasks.hello_avengercon\", ignore_result=False)\ndef hello_avengercon() -&gt; str:\n    return \"Hello, AvengerCon! &lt;3 Celery\"\n</code></pre> avengercon.celery.config.py<pre><code>include = [\n    \"avengercon.celery.tasks\",\n]\n</code></pre> celery.py<pre><code>from avengercon.celery import config\nfrom celery import Celery\n\ncelery_server: Celery = Celery(main=\"avengercon\")\ncelery_server.config_from_object(obj=config, silent=False, force=True)\n</code></pre> <p>When starting the Celery App, you'll see the tasks listed in the logs Terminal Logs<pre><code>avengercon-celery              |  -------------- celery@1f7940b050a2 v5.3.6 (emerald-rush)\navengercon-celery              | --- ***** -----\navengercon-celery              | -- ******* ---- Linux-6.5.0-21-generic-x86_64-with-glibc2.36 2024-02-27 15:39:34\navengercon-celery              | - *** --- * ---\navengercon-celery              | - ** ---------- [config]\navengercon-celery              | - ** ---------- .&gt; app:         avengercon:0x7f391a996250\navengercon-celery              | - ** ---------- .&gt; transport:   redis://:**@redis:6379/0\navengercon-celery              | - ** ---------- .&gt; results:     redis://:**@redis:6379/0\navengercon-celery              | - *** --- * --- .&gt; concurrency: 24 (prefork)\navengercon-celery              | -- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)\navengercon-celery              | --- ***** -----\navengercon-celery              |  -------------- [queues]\navengercon-celery              |                 .&gt; celery           exchange=celery(direct) key=celery\navengercon-celery              |\navengercon-celery              |\navengercon-celery              | [tasks]\navengercon-celery              |   . tasks.hello_avengercon\navengercon-celery              |\navengercon-celery              | [2024-02-27 15:39:35,400: INFO/MainProcess] Connected to redis://:**@redis:6379/0\navengercon-celery              | [2024-02-27 15:39:35,401: INFO/MainProcess] mingle: searching for neighbors\navengercon-celery              | [2024-02-27 15:39:36,405: INFO/MainProcess] mingle: all alone\n</code></pre></p>"},{"location":"workshop/5_beam/","title":"Apache Beam","text":""},{"location":"workshop/5_beam/#what-is-apache-beam","title":"What is Apache Beam","text":"<p>The Apache Beam Overview is a nice introduction to the framework. The very summarized explanation is it allows you to write code once, possibly in the language you already use, then run that code on \"production\" computation servers like Apache Flink, Spark, or Google Cloud Platform.</p> <p>There's four main benefits to using Apache Beam:</p> <ol> <li> <p>The SDK forces you to more-or-less correctly use map-shuffle-reduce programming patterns that's ready to scale to the moon .</p> </li> <li> <p>First class support for streaming data via windowing and other functionality make it one of the few frameworks that could be integrated into your organization's \"data firehose\" to apply AI/ML, heuristics, etc. to intelligently flag or retain valuable information that may otherwise go undetected/missed because the volume of data is too much to save into S3 or elsewhere.</p> </li> <li> <p>If your organization already runs a supported runner, then there's a fairly straightforward developer experience (DX) transitioning code you've written on your local computer to production. This DX is truly streamlined if you've got a dev cluster for your runner available that mimics your prod infrastructure.</p> <p>Linkage with GCP Cloud Dataflow</p> <p>Google is the primary contributor to Apache Beam as it underpins their Google Cloud Platform -- Cloud Dataflow service. Be aware that the transition from dev to prod may have more \"sharp edges\" when using runners that aren't Cloud Dataflow. That said, if you ARE using Cloud Dataflow then this transition is very smooth.</p> </li> <li> <p>While potentially restrictive, the available inputs and outputs for Beam helps you and your organization avoid pitfalls of trying to pair a technology (e.g. traditional SQL database like PostgreSQL) that can't gracefully horizontally scale with code that can.</p> </li> </ol> <p>There's also some significant drawbacks, particularly for Python developers using self- hosted runners like an Apache Spark or Apache Flink cluster.</p> <ol> <li> <p>The execution model for \"real\" Beam pipelines, particularly those not written in Java, isn't as trivial as the SDK may imply. See the execution diagram below.</p> </li> <li> <p>For Python devs, you're forever stuck with months old versions of Numpy, Pandas, and other dependencies as the <code>apache-beam</code> Python SDK has exact versions (ctrl+f \"pandas\") it requires.</p> </li> <li> <p>The pain points of keeping the Python environment used by Dask/Prefect/Celery workers synced with local dev environments are magnified by the complex intertwining of Python, Java, and infrastructure orchestrated by Beam.</p> </li> </ol>"},{"location":"workshop/5_beam/#tour-of-beam","title":"\"Tour of Beam\"","text":"<p>Right click and \"open in new tab\"</p> <p>Tour of Beam </p> <p>Tour of Beam is a fully hosted crash course maintained by the Beam authors and won't be running on our local system. Those that would like to try self-hosting are encouraged to instead look at the \"Beam Playground source code\" which includes a docker compose based deployment. As with the \"Tour of Beam source code\", the \"Beam Playground\" has a fully-hosted interactive browser-based interpreter to try out Apache Beam. \"Tour of Beam\" relies on \"Beam Playground\" and Google Cloud Platform (GCP).</p> <p></p>"},{"location":"workshop/5_beam/#real-use-of-apache-beam","title":"'Real' Use of Apache Beam","text":"<p>Complex Infrastructure</p> <p>While somewhat detailed and quirky, this workshop's code base and the previous sections demonstrate how Dask/Prefect and Celery can be deployed and used by a small team. This isn't really the case with Apache Beam. The \"big data\" Apache ecosystem projects like Flink, Spark, and Kafka are non-trivial to get running, much more effectively tune and maintain. The dataflow diagram below from the Beam authors hint at this significant infrastructure investment.</p>"},{"location":"workshop/5_beam/#example-orchestrating-apache-flink","title":"Example: Orchestrating Apache Flink","text":"<p>Apache Flink allows for parallel streaming processing of data from either files (batch) or true streaming (via message broker like Pub/Sub, Kafka, RabbitMQ, etc.).</p> <p>Two ways for python developers to leverage Flink is either indirectly via Apache Beam or the native apache-flink library. For pros and cons of using Apache Beam, see this and this article by Deepak Nagaraj. The complexity highlighted in these article is aluded to by the SDK Harness Config docs for Apache Beam which shows the need to package, maintain, and troubleshoot custom docker images for each execution environment a beam pipeline may try to execute. See the excerpt of this complex workflow for basic Apache Beam tasks below.</p> <p></p>"}]}